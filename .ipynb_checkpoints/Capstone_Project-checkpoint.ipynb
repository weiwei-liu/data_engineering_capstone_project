{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I94 US Immigration Data Analysis\n",
    "### Udacity Data Engineering Nanodegree Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project creates a data lake for US Immigration data in the year 2016. This data lake will be used for data analytics and BI purpose. Currently data is stored at S3. A data model with star schema will be designed and a ETL pipeline will be built to extract data from S3, clean up using Spark, and load back to S3 with Parquet format. And this processed data could be used to build SQL database.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import configparser\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, split\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DateType\n",
    "from pyspark.sql.functions import udf\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import sequence, to_date, explode, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.cfg']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AWS credentials from configure file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "SECRET = config['AWS']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "IMMI_DATA = config['S3']['IMMI_DATA']\n",
    "DEMO_DATA = config['S3']['DEMO_DATA']\n",
    "AIRPORT_DATA = config['S3']['AIRPORT_DATA']\n",
    "COUNTRY_DATA = config['S3']['COUNTRY_DATA']\n",
    "REGION_DATA = config['S3']['REGION_DATA']\n",
    "OUTPUT_PATH = config['S3-OUTPUT']['OUTPUT_PATH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = 'AKIA557Q562U5OP4FMW7'\n",
    "SECRET = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.2,saurfang:spark-sas7bdat:3.0.0-s_2.12\") \\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\") \\\n",
    "        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \\\n",
    "        .config('spark.hadoop.fs.s3a.access.key', KEY) \\\n",
    "        .config('spark.hadoop.fs.s3a.secret.key', SECRET) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Project Scope and Data Gathering\n",
    "\n",
    "#### Scope \n",
    "\n",
    "This project will create a data lake and clean up immigration, airport and demographics data collected from various sources, and load cleaned data back to S3 for a star schema database. This database could be used for data analyst to explore the relation among cities of US, point of entry and immigrant from different countries.\n",
    "\n",
    "Some of the examples this data model can solve would be:\n",
    "\n",
    "* What's the relation between immigration and demographics in different state? Would point of entry determine the race composition of that state?\n",
    "\n",
    "* What is the busiest week or month for immigration officer? at which airport?\n",
    "\n",
    "* Which country contributes the most for visitors?  \n",
    "\n",
    "The main tools used in this project are Python, Pandas, Pyspark, SQL, S3, Airflow and Redshift.\n",
    "\n",
    "#### Data Intro\n",
    "\n",
    "\n",
    "* [I94 Immigration Data](https://www.trade.gov/national-travel-and-tourism-office): This data comes from the US National Tourism and Trade Office.  \n",
    "\n",
    "    * Description:  \n",
    "    Detailed description can be found at [I94_SAS_Labels_Descriptions.SAS](./I94_SAS_Labels_Descriptions.SAS). This file contains descriptions for the I94 meta data. This data contains 12 files for 12 months immigration data in 2016.\n",
    "    Each file with the size of 500Mb and roughly has 3 million rows.\n",
    "    \n",
    "    * Sample:\n",
    "|         |       cicid |   i94yr |   i94mon |   i94cit |   i94res | i94port   |   arrdate |   i94mode | i94addr   |   depdate |   i94bir |   i94visa |   count |   dtadfile | visapost   |   occup | entdepa   | entdepd   |   entdepu | matflag   |   biryear |   dtaddto | gender   |   insnum | airline   |      admnum |   fltno | visatype   |\n",
    "|--------:|------------:|--------:|---------:|---------:|---------:|:----------|----------:|----------:|:----------|----------:|---------:|----------:|--------:|-----------:|:-----------|--------:|:----------|:----------|----------:|:----------|----------:|----------:|:---------|---------:|:----------|------------:|--------:|:-----------|\n",
    "| 1397736 | 2.86358e+06 |    2016 |        4 |      689 |      689 | MIA       |     20559 |         1 | FL        |     20569 |       10 |         2 |       1 |   20160415 | SPL        |     nan | G         | O         |       nan | M         |      2006 |  10142016 | F        |      nan | JJ        | 9.36397e+10 |   08094 | B2         |\n",
    "|  997880 | 2.03818e+06 |    2016 |        4 |      245 |      245 | SFR       |     20555 |         1 | CA        |     20570 |       50 |         2 |       1 |   20160411 | SHG        |     nan | G         | O         |       nan | M         |      1966 |  10102016 | F        |      nan | MU        | 9.32671e+10 |   00589 | B2         |\n",
    "\n",
    "\n",
    "* [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "    * Description:\n",
    "    This dataset contains information about the demographics of all US cities and census-designated places with a population greater or equal to 65,000. This data comes from the US Census Bureau's 2015 American Community Survey.\n",
    "    * Sample:\n",
    "    |      | City         | State      |   Median Age |   Male Population |   Female Population |   Total Population |   Number of Veterans |   Foreign-born |   Average Household Size | State Code   | Race               |   Count |\n",
    "|-----:|:-------------|:-----------|-------------:|------------------:|--------------------:|-------------------:|---------------------:|---------------:|-------------------------:|:-------------|:-------------------|--------:|\n",
    "|  629 | Arden-Arcade | California |         41.5 |             47596 |               48680 |              96276 |                 6511 |          13458 |                     2.18 | CA           | White              |   69369 |\n",
    "| 1438 | Edmond       | Oklahoma   |         32.5 |             45191 |               44899 |              90090 |                 5006 |           5585 |  \n",
    "    \n",
    "\n",
    "\n",
    "* [Airport Code Table](https://datahub.io/core/airport-codes#data) \n",
    "\n",
    "    * Description:\n",
    "    This is a simple table of airport codes and corresponding cities. \n",
    "    * Sample\n",
    "    |       | ident   | type          | name                   |   elevation_ft |   continent | iso_country   | iso_region   | municipality   | gps_code   |   iata_code | local_code   | coordinates                           |\n",
    "|------:|:--------|:--------------|:-----------------------|---------------:|------------:|:--------------|:-------------|:---------------|:-----------|------------:|:-------------|:--------------------------------------|\n",
    "|  1613 | 16Z     | seaplane_base | Mc Grath Seaplane Base |            325 |         nan | US            | US-AK        | Mcgrath        | 16Z        |         nan | 16Z          | -155.593002319, 62.9580001831         |\n",
    "| 10090 | 9NC7    | small_airport | Willow Creek Airport   |            572 |         nan | US            | US-NC        | Mt Pleasant    | 9NC7       |         nan | 9NC7         | -80.44000244140625, 35.36970138549805 |\n",
    "\n",
    "* [ISO Country Code](https://datahub.io/core/country-list)\n",
    "\n",
    "    * Description:\n",
    "    ISO 3166-1-alpha-2 English country names and code elements. This list states the country names (official short names in English) in alphabetical order as given in ISO 3166-1 and the corresponding ISO 3166-1-alpha-2 code elements. [ISO 3166-1].\n",
    "    * Sample\n",
    "    \n",
    "    |     | Name                                         | Code   |\n",
    "|----:|:---------------------------------------------|:-------|\n",
    "|   0 | Afghanistan                                  | AF     |\n",
    "|   1 | Åland Islands                                | AX     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### 2.1 Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 05:16:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## read airport data from s3\n",
    "airport_data = spark.read.format('csv').load(AIRPORT_DATA,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## change spark df to pandas df\n",
    "ap_df = airport_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>None</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435</td>\n",
       "      <td>NA</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>None</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                  name elevation_ft continent  \\\n",
       "0   00A       heliport     Total Rf Heliport           11        NA   \n",
       "1  00AA  small_airport  Aero B Ranch Airport         3435        NA   \n",
       "\n",
       "  iso_country iso_region municipality gps_code iata_code local_code  \\\n",
       "0          US      US-PA     Bensalem      00A      None        00A   \n",
       "1          US      US-KS        Leoti     00AA      None       00AA   \n",
       "\n",
       "                          coordinates  \n",
       "0  -74.93360137939453, 40.07080078125  \n",
       "1              -101.473911, 38.704022  "
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ap_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ident               0\n",
       "type                0\n",
       "name                0\n",
       "elevation_ft     7006\n",
       "continent           0\n",
       "iso_country         0\n",
       "iso_region          0\n",
       "municipality     5676\n",
       "gps_code        14045\n",
       "iata_code       45886\n",
       "local_code      26389\n",
       "coordinates         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## null value count in each column\n",
    "ap_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check if there is duplicated rows\n",
    "ap_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ap_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are total 55075 rows in the airports data, column `gps_code`,`iata_code` and `local_code` have a lot missing values, while `elevation_ft` and `municipality` also has a few missing values. Generally there is no duplicates rows in the data. Besides, the `coordinates` columns contains latitude and longitude information,this could be diveded into 2 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read airport data from s3\n",
    "demo_data = spark.read.format('csv').load(DEMO_DATA,header=True,sep=';')\n",
    "\n",
    "## change spark df to pandas df\n",
    "demo_df = demo_data.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601</td>\n",
       "      <td>41862</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562</td>\n",
       "      <td>30908</td>\n",
       "      <td>2.6</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129</td>\n",
       "      <td>49500</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147</td>\n",
       "      <td>32935</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City          State Median Age Male Population Female Population  \\\n",
       "0  Silver Spring       Maryland       33.8           40601             41862   \n",
       "1         Quincy  Massachusetts       41.0           44129             49500   \n",
       "\n",
       "  Total Population Number of Veterans Foreign-born Average Household Size  \\\n",
       "0            82463               1562        30908                    2.6   \n",
       "1            93629               4147        32935                   2.39   \n",
       "\n",
       "  State Code                Race  Count  \n",
       "0         MD  Hispanic or Latino  25924  \n",
       "1         MA               White  58723  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 2891 rows in demographacis data frame.\n",
      "\n",
      "NULL Value counts:\n",
      " \n",
      " City                       0\n",
      "State                      0\n",
      "Median Age                 0\n",
      "Male Population            3\n",
      "Female Population          3\n",
      "Total Population           0\n",
      "Number of Veterans        13\n",
      "Foreign-born              13\n",
      "Average Household Size    16\n",
      "State Code                 0\n",
      "Race                       0\n",
      "Count                      0\n",
      "dtype: int64\n",
      "\n",
      "Duplicates count:  0\n"
     ]
    }
   ],
   "source": [
    "print(f'There are total {len(demo_df)} rows in demographacis data frame.\\n')\n",
    "\n",
    "## null value count in each column\n",
    "print('NULL Value counts:\\n \\n',demo_df.isnull().sum())\n",
    "\n",
    "## check if there is duplicated rows\n",
    "print('\\nDuplicates count: ', demo_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are total 2891 rows in the demographics data, column `Number of Veterans`,`Foreign-born` and `Average Household Size` have a few missing values. Generally there is no duplicates rows in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the immigration data is really large, this sample will on read one month data to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read airport data from s3\n",
    "immi_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(IMMI_DATA+'/i94_jun16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total 3574989 rows in the data of June.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'There are total {immi_data.count()} rows in the data of June.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|  4.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  59.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      Z|   null|      U|   null| 1957.0|10032016|  null|  null|   null|1.4938462027E10| null|      WT|\n",
      "|  5.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|   null|   null|  50.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      Z|   null|      U|   null| 1966.0|10032016|  null|  null|   null|1.7460063727E10| null|      WT|\n",
      "|  6.0|2016.0|   6.0| 213.0| 213.0|    XXX|20609.0|   null|   null|   null|  27.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      T|   null|      U|   null| 1989.0|     D/S|  null|  null|   null|  1.679297785E9| null|      F1|\n",
      "|  7.0|2016.0|   6.0| 213.0| 213.0|    XXX|20611.0|   null|   null|   null|  23.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      T|   null|      U|   null| 1993.0|     D/S|  null|  null|   null|  1.140963185E9| null|      F1|\n",
      "| 16.0|2016.0|   6.0| 245.0| 245.0|    XXX|20632.0|   null|   null|   null|  24.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null|    null| null|      T|   null|      U|   null| 1992.0|     D/S|  null|  null|   null|  1.934535285E9| null|      F1|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|   520|     0|      0|      0|  61187| 186064| 287071|   639|      0|    0|       0|          0|          0|         0|          0|            0|      10| 2105106|3564623|   1629| 343368|3574276| 284603|    639|  61668|582193|3457964| 158511|     0|72296|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## check null values\n",
    "immi_data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in immi_data.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## check duplicates\n",
    "if immi_data.count() > immi_data.dropDuplicates().count():\n",
    "    raise ValueError('Data has duplicates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are total 3574989 rows in the Immigration data of June, quite a few columns have missing values. Generally there is no duplicates rows in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Data Cleaning\n",
    "\n",
    "Clean and transform data to make sure the data is ready for downstream extract and save."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Airport Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read airport data from s3\n",
    "airport_data = spark.read.format('csv').load(AIRPORT_DATA,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data = airport_data.na.fill({'municipality': 'NA'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Country Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read country data from s3\n",
    "country_data = spark.read.format('csv').load(COUNTRY_DATA,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+\n",
      "|          Name|Code|\n",
      "+--------------+----+\n",
      "|   Afghanistan|  AF|\n",
      "| Åland Islands|  AX|\n",
      "|       Albania|  AL|\n",
      "|       Algeria|  DZ|\n",
      "|American Samoa|  AS|\n",
      "+--------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Region Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read region data from s3\n",
    "region_data = spark.read.format('csv').load(REGION_DATA,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+-----+----------+\n",
      "|country_code|subdivision_name| code|state_code|\n",
      "+------------+----------------+-----+----------+\n",
      "|          US|         Alabama|US-AL|        AL|\n",
      "|          US|          Alaska|US-AK|        AK|\n",
      "|          US|         Arizona|US-AZ|        AZ|\n",
      "|          US|        Arkansas|US-AR|        AR|\n",
      "|          US|      California|US-CA|        CA|\n",
      "+------------+----------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read demographics data from s3\n",
    "demo_data = spark.read.format('csv').load(DEMO_DATA,header=True,sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data = demo_data.na.fill({\\\n",
    "                              'City':'NA', \\\n",
    "                              'State':'NA', \\\n",
    "                              'Median Age':0.0, \\\n",
    "                              'Male Population':0.0, \\\n",
    "                              'Female Population':0.0, \\\n",
    "                              'Total Population':0.0, \\\n",
    "                              'Number of Veterans':0.0, \\\n",
    "                              'Foreign-born':0.0, \\\n",
    "                              'Average Household Size':0.0, \\\n",
    "                              'State Code':'NA', \\\n",
    "                              'Race':'NA', \\\n",
    "                              'Count':0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill null with NA\n",
    "immi_data = immi_data.na.fill('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|  4.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|     NA|   null|  59.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|      NA|      NA|   NA|      Z|     NA|      U|     NA| 1957.0|10032016|    NA|    NA|     NA|1.4938462027E10|   NA|      WT|\n",
      "|  5.0|2016.0|   6.0| 135.0| 135.0|    XXX|20612.0|   null|     NA|   null|  50.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|      NA|      NA|   NA|      Z|     NA|      U|     NA| 1966.0|10032016|    NA|    NA|     NA|1.7460063727E10|   NA|      WT|\n",
      "|  6.0|2016.0|   6.0| 213.0| 213.0|    XXX|20609.0|   null|     NA|   null|  27.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|      NA|      NA|   NA|      T|     NA|      U|     NA| 1989.0|     D/S|    NA|    NA|     NA|  1.679297785E9|   NA|      F1|\n",
      "|  7.0|2016.0|   6.0| 213.0| 213.0|    XXX|20611.0|   null|     NA|   null|  23.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|      NA|      NA|   NA|      T|     NA|      U|     NA| 1993.0|     D/S|    NA|    NA|     NA|  1.140963185E9|   NA|      F1|\n",
      "| 16.0|2016.0|   6.0| 245.0| 245.0|    XXX|20632.0|   null|     NA|   null|  24.0|    3.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|      NA|      NA|   NA|      T|     NA|      U|     NA| 1992.0|     D/S|    NA|    NA|     NA|  1.934535285E9|   NA|      F1|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The main focus of this project is the immigration and demographics information. It would be natural to build a snowflake schema data model. The immigration and demographics table as the fact tables, and with dimensional tables like time, city, country, and airport.\n",
    "\n",
    "![](schema_diagram.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "* Read in configuration settings (dl.cfg);\n",
    "* Using Spark df to read in raw data from S3, then clean and transformed data, finally select the coresponding data for each fact and dimensional table, and save table data onto S3 for future use.\n",
    "* Finally, data quality checks are run for each table to validate the output (key columns don't have nulls, record of each table is not 0,etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### airport table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data.filter(col('iso_country')=='US').createOrReplaceTempView(\"staging_airport_table\")\n",
    "airport_table = spark.sql(\"\"\"SELECT\n",
    "                                ident        AS airport_id,\n",
    "                                name         AS airport_name,\n",
    "                                type         AS airport_type,\n",
    "                                iso_region   AS region,\n",
    "                                municipality AS municipality,\n",
    "                                CAST(SPLIT(coordinates,',')[0] AS double)  AS coordinate_x,\n",
    "                                CAST(SPLIT(coordinates,',')[1] AS double)  AS coordinate_y\n",
    "                         FROM staging_airport_table ORDER BY region\n",
    "                        \"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------+------+------------+------------------+------------------+\n",
      "|airport_id|        airport_name| airport_type|region|municipality|      coordinate_x|      coordinate_y|\n",
      "+----------+--------------------+-------------+------+------------+------------------+------------------+\n",
      "|      01AR|Community Hospita...|     heliport| US-AR|    De Queen|-94.35489654541016|34.047298431396484|\n",
      "|      01WT|    Odyssey Heliport|     heliport| US-WA|      Renton|       -122.210908|         47.518178|\n",
      "|      02NJ|     Penske Heliport|     heliport| US-NJ|  Piscataway|-74.46710205078125| 40.55730056762695|\n",
      "|      07CT|        Tnt Heliport|       closed| US-CT|       Salem|-72.24960327148438| 41.45289993286133|\n",
      "|      07FA|Ocean Reef Club A...|small_airport| US-FL|   Key Largo|  -80.274803161621|   25.325399398804|\n",
      "+----------+--------------------+-------------+------+------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "airport_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## write table to parquet format\n",
    "\n",
    "airport_table.write.mode('overwrite').partitionBy('region').parquet(OUTPUT_PATH + 'airport.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_data.createOrReplaceTempView(\"staging_country_table\")\n",
    "country_table = spark.sql(\"\"\"SELECT\n",
    "                                Name        AS country_name,\n",
    "                                Code         AS country_code\n",
    "                         FROM staging_country_table ORDER BY country_name\n",
    "                        \"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+\n",
      "|        country_name|country_code|\n",
      "+--------------------+------------+\n",
      "|          Antarctica|          AQ|\n",
      "|              Sweden|          SE|\n",
      "|           Indonesia|          ID|\n",
      "|United Arab Emirates|          AE|\n",
      "|          Azerbaijan|          AZ|\n",
      "+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "country_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## write table to parquet format\n",
    "\n",
    "country_table.write.mode('overwrite').parquet(OUTPUT_PATH + 'country.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Region Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data = region_data.filter(col('country_code')=='US').withColumn('state_code',split(col('code'),'-')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data.createOrReplaceTempView(\"staging_region_table\")\n",
    "region_table = spark.sql(\"\"\"SELECT\n",
    "                                subdivision_name        AS state_name,\n",
    "                                state_code              AS state_code,\n",
    "                                country_code            AS country_code,\n",
    "                                code                    AS country_state\n",
    "                         FROM staging_region_table ORDER BY state_name\n",
    "                        \"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------------+-------------+\n",
      "|   state_name|state_code|country_code|country_state|\n",
      "+-------------+----------+------------+-------------+\n",
      "|      Montana|        MT|          US|        US-MT|\n",
      "|West Virginia|        WV|          US|        US-WV|\n",
      "|       Alaska|        AK|          US|        US-AK|\n",
      "| Pennsylvania|        PA|          US|        US-PA|\n",
      "|       Nevada|        NV|          US|        US-NV|\n",
      "+-------------+----------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "region_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## write table to parquet format\n",
    "\n",
    "region_table.write.mode('overwrite').parquet(OUTPUT_PATH + 'region.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demographics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_data.createOrReplaceTempView(\"staging_demo_table\")\n",
    "demo_table = spark.sql(\"\"\"SELECT\n",
    "                                City            AS city_name,\n",
    "                                `State Code`    AS state_code,\n",
    "                                `Median Age`    AS median_age,\n",
    "                                `Male Population` AS male_population,\n",
    "                                `Female Population` AS female_population,\n",
    "                                `Total Population`  AS total_population,\n",
    "                                `Foreign-born`      AS foreign_born,\n",
    "                                `Average Household Size` AS avg_household_size,\n",
    "                                `Race`          AS race,\n",
    "                                `Count`         AS race_population\n",
    "                         FROM staging_demo_table\n",
    "                        \"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+---------------+-----------------+----------------+------------+------------------+--------------------+---------------+\n",
      "| city_name|state_code|median_age|male_population|female_population|total_population|foreign_born|avg_household_size|                race|race_population|\n",
      "+----------+----------+----------+---------------+-----------------+----------------+------------+------------------+--------------------+---------------+\n",
      "|  O'Fallon|        MO|      36.0|          41762|            43270|           85032|        3269|              2.77|  Hispanic or Latino|           2583|\n",
      "|   Buffalo|        NY|      33.1|         124537|           133529|          258066|       24630|              2.27|  Hispanic or Latino|          29656|\n",
      "|Costa Mesa|        CA|      34.8|          59097|            54089|          113186|       26645|              2.59|               Asian|           9165|\n",
      "|  Columbia|        MO|      26.8|          56544|            62554|          119098|       10729|              2.37|Black or African-...|          15489|\n",
      "|   Warwick|        RI|      42.6|          39289|            42409|           81698|        5318|               2.4|  Hispanic or Latino|           4723|\n",
      "+----------+----------+----------+---------------+-----------------+----------------+------------+------------------+--------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## write table to parquet format\n",
    "\n",
    "demo_table.write.mode('overwrite').partitionBy('state_code').parquet(OUTPUT_PATH + 'demographics.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Immigration Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read airport data from s3\n",
    "immi_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(IMMI_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi_data = immi_data.na.fill({'depdate':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertTimeUDF = udf(lambda z: convert_datetime(z),DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "convertTimeUDF = udf(lambda z: datetime(1960,1,1)+timedelta(days=z),DateType()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi_data = immi_data.withColumn('arrdate',convertTimeUDF(col('arrdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi_data = immi_data.withColumn('depdate',convertTimeUDF(col('depdate')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 05:32:35 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "immi_data.createOrReplaceTempView(\"staging_immi_table\")\n",
    "immi_table = spark.sql(\"\"\"SELECT\n",
    "                                cicid            AS immi_id,\n",
    "                                i94res           AS residency,\n",
    "                                i94port          AS entry_port,\n",
    "                                arrdate          AS arrival_date,\n",
    "                                i94mode          AS transportation,\n",
    "                                i94addr          AS arrival_state,\n",
    "                                depdate          AS departure_date,\n",
    "                                i94bir           AS age,\n",
    "                                i94visa          AS travel_purpose,\n",
    "                                biryear          AS birth_year,\n",
    "                                gender           AS gender,\n",
    "                                airline          AS airline,\n",
    "                                fltno            AS flight_number,\n",
    "                                visatype         AS visa_type\n",
    "                         FROM staging_immi_table\n",
    "                        \"\"\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+------------+--------------+-------------+--------------+----+--------------+----------+------+-------+-------------+---------+\n",
      "|immi_id|residency|entry_port|arrival_date|transportation|arrival_state|departure_date| age|travel_purpose|birth_year|gender|airline|flight_number|visa_type|\n",
      "+-------+---------+----------+------------+--------------+-------------+--------------+----+--------------+----------+------+-------+-------------+---------+\n",
      "|  158.0|    103.0|       NEW|  2016-06-01|           1.0|           NV|    2016-06-05|38.0|           2.0|    1978.0|     M|     LX|        00018|       WT|\n",
      "|  293.0|    103.0|       LOS|  2016-06-01|           1.0|           CA|    2016-06-21|68.0|           2.0|    1948.0|     M|     AA|        00109|       WT|\n",
      "|  424.0|    103.0|       SFR|  2016-06-01|           1.0|           CA|    2016-06-15|68.0|           2.0|    1948.0|     F|     UA|        00059|       WT|\n",
      "|  601.0|    104.0|       ATL|  2016-06-01|           1.0|           LA|    1960-01-01|33.0|           2.0|    1983.0|  null|     DL|        00085|       WT|\n",
      "| 1582.0|    105.0|       CHI|  2016-06-01|           1.0|           NV|    2016-07-23|31.0|           2.0|    1985.0|     M|     OS|        00065|       B2|\n",
      "+-------+---------+----------+------------+--------------+-------------+--------------+----+--------------+----------+------+-------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "immi_table.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_data = spark.sql(\"SELECT sequence(to_date('2015-12-01'), to_date('2022-01-01'), interval 1 day) as date\").withColumn(\"date\", explode(col(\"date\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|      date|\n",
      "+----------+\n",
      "|2015-12-01|\n",
      "|2015-12-02|\n",
      "|2015-12-03|\n",
      "|2015-12-04|\n",
      "|2015-12-05|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_data.createOrReplaceTempView(\"staging_date_table\")\n",
    "date_table = spark.sql(\"\"\"SELECT           \n",
    "                                 date              AS date,\n",
    "                                 day(date)         AS day,\n",
    "                                 weekofyear(date)  AS week,\n",
    "                                 month(date)       AS month,\n",
    "                                 year(date)        AS year,\n",
    "                                 dayofweek(date)   AS weekday\n",
    "                          FROM staging_date_table\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----+----+-------+\n",
      "|      date|day|week|month|year|weekday|\n",
      "+----------+---+----+-----+----+-------+\n",
      "|2015-12-05|  5|  49|   12|2015|      7|\n",
      "|2016-01-25| 25|   4|    1|2016|      2|\n",
      "|2016-02-17| 17|   7|    2|2016|      4|\n",
      "|2016-04-08|  8|  14|    4|2016|      6|\n",
      "|2016-05-01|  1|  17|    5|2016|      1|\n",
      "+----------+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "date_table.sample(0.05).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "## write table to parquet format\n",
    "date_table.write.mode('overwrite').parquet(OUTPUT_PATH + 'date.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "\n",
    "Data quality checks ensure the pipeline ran as expected. Many of the Quality Checks were perfomed earlier to replace missing values, removing duplicates etc. Another important check is to make sure the primary key of the table is not null and unique before they are written back to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_null_checks(table,column):\n",
    "\n",
    "    table.createOrReplaceTempView(\"temp\")\n",
    "    \n",
    "\n",
    "    null_count = table.filter(col(column).isNull()).count()\n",
    "    exp_result = 0\n",
    "\n",
    "    if exp_result != null_count:\n",
    "        print(f'There are null values in the table!')\n",
    "    else:\n",
    "        print('Data Quality checks passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_unique_checks(table,column):\n",
    "\n",
    "\n",
    "    unique_count = table.select(column).distinct().count()\n",
    "    row_count = table.count()\n",
    "\n",
    "    if unique_count != row_count:\n",
    "        print(f'There are duplicated values in the table')\n",
    "    else:\n",
    "        print('Data Quality checks passed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 190:====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality checks passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_null_checks(immi_table,'immi_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 204:==================================>                      (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality checks passed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_unique_checks(immi_table,'immi_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 ETL process result sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data lake created could be used for analyzing immigration patterns. For example:\n",
    "\n",
    "*Which week has the most visitors in June?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:30 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "21/11/24 05:42:31 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "[Stage 40:===========>                                              (1 + 4) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|week| count|\n",
      "+----+------+\n",
      "|  26|520705|\n",
      "|  22|596482|\n",
      "|  23|797060|\n",
      "|  25|859081|\n",
      "|  24|801661|\n",
      "+----+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "immi_table.join(date_table,immi_table.arrival_date == date_table.date,'left').groupby(date_table.week).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in June,2016, there 5 weeks in this month, and it starts from week 22, and ends at week 26. Week 22 and 26 has relatively low number of visitors, while Week 23,24,and 25 have high volume of visitors, with week 25 top at ~859k visitor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "**Airport Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:-------------------|:-----------------------------|\n",
    "| airport_id   | 01NM                   | varchar               | identifier                         |\n",
    "| airport_name | Champion Ranch Airport | varchar    | name of the airport |\n",
    "| airport_type | small_airport          | varchar      | type of the airport                    |\n",
    "| region       | US-NM                  | varchar              | iso region code                        |\n",
    "| municipality  | Artesia                | varchar       | municipality of the airport                      |\n",
    "| coordinate_x | -104.540278            | double | latitude|\n",
    "| coordinate_y | 33.008611              | double | longitude          |\n",
    "\n",
    "\n",
    "**Country Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:---------------------------|:--------|\n",
    "| country_name | Cayman Islands | varchar | name of country |\n",
    "| country_code | KY             | varchar     | iso country alpha-2 code   |\n",
    "\n",
    "**Region Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:---------------------------|:--------|\n",
    "| state_name    | Iowa  | varchar | name of US states |\n",
    "| state_code    | IA    | varchar     | code of the state         |\n",
    "| country_code  | US    | varchar     | iso country code         |\n",
    "| country_state | US-IA | varchar  | iso region code      |\n",
    "\n",
    "\n",
    "**Demographics Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:---------------------------|:--------|\n",
    "| city_name          | Brownsville | varchar                  | city name |\n",
    "| state_code         | TX          | varchar                        | code of the state where city is         |\n",
    "| median_age         | 30.6        | double                      | median age of the city       |\n",
    "| male_population    | 87689       | int                    | male population of the city      |\n",
    "| female_population  | 96199       | int                    | female population of the city      |\n",
    "| total_population   | 183888      | int                    | total population of the city     |\n",
    "| foreign_born       | 53301       | int                     | foreign born population of the city       |\n",
    "| avg_household_size | 3.48        | double                      | average household size of the city       |\n",
    "| race               | Asian       | varchar                  | races      |\n",
    "| race_population    | 1589        | int                    | population of the race       |\n",
    "\n",
    "\n",
    "**Immigration Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:---------------------------|:--------|\n",
    "| immi_id        | 4554704  | int  | identifier  |\n",
    "| residency      | 112      | int      | country code in SAS description file      |\n",
    "| entry_port     | SEA        | varchar        | entry port code in SAS description file        |\n",
    "| arrival_date   | 2016-06-23 | datetime | arrival date |\n",
    "| transportation | 1.0        | int        | means of transportation to US code in the SAS description file         |\n",
    "| arrival_state  | OR         | varchar         | state code of US         |\n",
    "| departure_date | 2016-06-30 | datetime | departure date |\n",
    "| age            | 15       | int       | age of the visitor       |\n",
    "| travel_purpose  | 2.0        | int        | travel purpose code in the SAS description file        |\n",
    "| birth_year     | 2001     | int     | birth year of the visitor     |\n",
    "| gender         | F          | char          | gender of the visitor          |\n",
    "| airline        | DL         | varchar         | airline taken by visitor         |\n",
    "| flight_number  | 00145      | varchar      | flight number of the flight      |\n",
    "| visa_type      | WT         | varchar         | visa type of the visitor         |\n",
    "\n",
    "**Date Table**\n",
    "\n",
    "|    field_name          |example    | field_type                   | description               |\n",
    "|:-------------|:-----------------------|:---------------------------|:--------|\n",
    "| date    | 2019-05-02 | datetime | date |\n",
    "| day     | 2          | int         |     day     |\n",
    "| week    | 18         | int         | which week the day falls in the year          |\n",
    "| month   | 5          | int          | month          |\n",
    "| year    | 2019       | int       | year       |\n",
    "| weekday | 5          | int          | which day is the day in a week          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Technology Choice\n",
    "\n",
    "This project is designed to build a data lake for Analytics and BI teams to research the immigration pattern, country of origin, airlines and its relation with different state's demographics. Creating data lake on S3 seemed appropriate choice, it offers flexibility, efficiency and cost effectiveness. S3 data lake offers a convenient way for data scientist to use the data directly, or as a staging area for data engineers to build dimensional tables for REDSHIFT OR RDS.\n",
    "\n",
    "Python, Spark framework and Pyspark are appropriate choice for implementation because of the library support. There could be millions of data records for the immigration data every month, to handle this amount of data it is proper to use distributed system like Spark. Besides, with the data growing each month, the need for a distributed system becomes more obvious.\n",
    "\n",
    "ETL pipeline is scripted with etl.py, it can real immigration SAS files from S3 using spark session and after clean and transform, join with several other data sources, then the results are writen back to S3 in parquet format.\n",
    "\n",
    "\n",
    "\n",
    "#### 5.2 Data Update\n",
    "\n",
    "The I94 immigration data could be updated daily, weekly, or monthly, depending on the analytics or BI purpose. If it is research analysis, probably monthly or yearly would be enough. If it is for BI monitoring purpose, it might need to be update daily.\n",
    "\n",
    "However, other data is relatively more static (airport table, demographics table), they can be updated yearly or even serval year a time.\n",
    "\n",
    "#### 5.3 Different Scenarios Discussion:\n",
    "\n",
    "* The data was increased by 100x.\n",
    "\n",
    "Right now there are around 6 gigabytes of data, and if the data increased by 100x, there would be 600Gb data. It could still be handled with distributed Spark system in EMR. If speed is the concern, the nodes or clusters could be increased easily with EMR.\n",
    "\n",
    "* The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "\n",
    "Apache Airflow can be a proper solution for schedueled scripts runs.\n",
    "\n",
    "* The database needed to be accessed by 100+ people.\n",
    "\n",
    "Build the database in Redshift could help to provide access for hundreds of people to access the database at the same.\n",
    "It says that Redshift can handle 500 concurrent connections to a Redshift cluster and maximum 15 queries can be run at the same time in a cluster. If more concurrent queries are needed, redshift provides Concurrency Scaling feature, where you can support virtually unlimited concurrent users and concurrent queries, with consistently fast query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
